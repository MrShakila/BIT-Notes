# Software Testing Process: A Comprehensive Overview

## The Software Testing Process

### Defining the Software Testing Process

- Effective and efficient testing involves proper planning and execution of tests designed to cover all necessary system areas.

- Results should be reviewed regularly.

- This is a process with identifiable tasks and activities.

- There's no one-size-fits-all approach; the right process achieves objectives efficiently.

- The best process for one project may not be suitable for another.

### Factors Influencing the Testing Process

- Software development life cycle models and project methodologies significantly impact the testing process.  An agile mobile app project will have a different process than a medical device project.

- Test levels and types influence the process. A large, complex project with various integration tests will require a more complex process.

- Product and project risks affect process formality; lower risk allows for less formal processes.

- Business domain, operational constraints (budgets, resources, timescales, complexity), organizational policies and practices, and required internal and external standards all play a role.

## Test Activities and Tasks

### Main Groups of Test Activities

- Test planning involves defining testing objectives and the approach to meet them within project constraints.  This includes deciding on techniques, tasks, and schedules.

- Test monitoring and control involves checking the status of testing activities, identifying variances from plans, and reporting to stakeholders.  Stopping testing or the project may be necessary if serious problems arise. Exit criteria, or "definition of done" in Agile, helps monitor progress.

- Test analysis identifies test conditions by analyzing the test basis (requirements, design, implementation, risk analysis).  It transforms general objectives into tangible test conditions and identifies defects like ambiguities or inconsistencies.

- Test design derives and specifies test cases from test conditions, addressing "how to test" by defining inputs and data needed for each test condition.  It refines general things to test into specifics for the component or system.

### Detailed Breakdown of Test Activities

- Test planning includes deciding on suitable test techniques, identifying tasks, and formulating a test schedule.

- Test monitoring and control uses exit criteria to track progress and involves corrective actions (test control) to address deviations from the plan.

- Test analysis involves analyzing the test basis for the appropriate test level, evaluating for defects, identifying features and sets of features to test, and prioritizing test conditions.  It also helps validate whether requirements capture stakeholder needs.

- Test design includes designing and prioritizing test cases, identifying necessary test data, designing the test environment, and capturing bi-directional traceability between test basis, conditions, cases, and procedures.

### Test Implementation and Execution

- Test implementation prepares the testware (work products for planning, designing, executing, evaluating, and reporting) needed for test execution.  It specifies test procedures and sets up the test environment.

- Test execution involves running tests and producing actual results.  Test suites (sets of test cases or procedures) are run according to the test execution schedule.

- Test execution includes recording item identities and versions, executing tests manually or automatically, comparing actual and expected results, analyzing anomalies (differences between actual and expected results), and reporting defects.

- Test completion makes test assets available for later use, leaves the test environment in good condition, and communicates results to stakeholders.  It involves closing defect reports, creating a test summary report, and archiving test assets for reuse.

### Test Work Products

- Work products are documentation, informal communication, or artifacts used in testing.  There is significant variation in types, organization, and naming.

- Examples include test plans, test reports, test conditions, test cases, test procedures, test suites, test execution schedules, defect reports, and test summary reports.

## Traceability Between Test Basis and Test Work Products

### Traceability and Bi-directional Traceability

- Traceability is the degree to which relationships can be established between work products.

- Bi-directional traceability exists between all test work products, allowing for analysis of change impacts, auditing, and measurement of coverage.

### Benefits of Good Traceability

- Analyzing the impact of changes.

- Making testing auditable and measurable.

- Meeting IT governance criteria.

- Improving the coherence of test progress reports.

- Relating technical aspects of testing to stakeholders in understandable terms.

- Providing information to assess product quality, process capability, and project progress.

## Software Development and Software Testing

### The Relationship Between Software Development and Testing

- The software development life cycle model significantly impacts testing.

- Testing organization must fit the development life cycle to deliver benefits.

- For every development activity, there's a corresponding test activity.

- Each test level has specific objectives.

- Test analysis and design should begin during the corresponding development activity.

- Testers should participate in discussions to define and refine requirements and design, reviewing work products as drafts become available.

### Sequential Development Models (Waterfall and V-Model)

- The Waterfall model has testing at the end, making late defect detection a problem. Feedback is difficult, and iterations are challenging.

- The V-model pairs each development phase with a corresponding testing phase, allowing for early testing and demonstrating that testing is not just execution-based.  A common V-model uses four test levels: component/unit, integration, system, and acceptance.

### Iterative and Incremental Development Models

- Most test tasks from sequential models are present in iterative and incremental models, but timing and extent may vary.

- Tasks may not be sequential.

- Fewer entry and exit criteria exist between activities compared to sequential models.

- Common issues include more regression testing, defects outside iteration scope, and less thorough testing.  Examples of iterative and incremental models include RUP, Scrum, Kanban, Spiral, Agile, and XP.

### Agile Development and Testing

- Scrum and XP emphasize testing throughout the process.

- Each iteration culminates in a short testing period, often with independent and business representatives.

- Developers write and run tests, using tools to automate and measure coverage.

- Agile offers benefits (focus on working software, simpler design, test-driven development) and challenges (constant time pressure, increased regression testing importance, less formal test design).  Testers often act as coaches.

### Choosing a Development Model

- Selecting a suitable development model is crucial based on the specific situation or context.

- The context determines test levels and activities.

## Software Testing Methodologies

### White-Box Testing

- Also known as structural, glass-box, or clear-box testing.

- Based on analysis of internal structure (code, data structures, design).

- Code is visible to testers.

- Involves testing for internal security holes, broken code paths, input flow, expected output, conditional loops, and individual statements, objects, and functions.

### White-Box Testing Techniques

- Statement coverage: traversing all statements at least once.

- Decision coverage: reporting true/false outcomes of Boolean expressions.

- Branch coverage: traversing each branch from decision points.

- Condition coverage: covering all individual conditions.

- Multiple condition coverage: testing all possible combinations of condition outcomes.

- Finite state machine coverage: checking visited and transited states and sequences.

- Path coverage: testing all possible paths, covering statements and branches.

### Black-Box Testing

- Also known as behavioral testing.

- Tests functionalities without knowledge of internal structure.

- Focuses on input and output, based on requirements and specifications.

### Types of Black-Box Testing

- Functional testing: related to functional requirements.

- Non-functional testing: related to non-functional requirements (performance, scalability, usability).

- Regression testing: done after code changes to ensure no new defects are introduced.

### Grey-Box Testing

- Tests with partial knowledge of internal structure.

- Identifies defects due to improper code structure or application use.

## Functional, Non-Functional, and Change-Related Testing

### Functional Testing

- Evaluates compliance with functional requirements.

- Tests are based on functions described in documents or understood by testers.

- Performed at all test levels.

- Can focus on suitability, interoperability, security, accuracy, and compliance.

- Two perspectives: requirement-based (using specifications) and business process-based (using knowledge of business processes and use cases).

- Thoroughness is measured by coverage of functions.

- Specialized skills may be needed for specific application domains.

### Non-Functional Testing

- Tests quality characteristics (non-functional attributes).

- Performed at all test levels.

- Includes performance testing (speed, response time, stability, reliability, scalability, resource usage) and load testing (system loading capacity).

- Also includes stress testing (stability and reliability under heavy load), usability testing (ease of use), portability testing (ease of moving between environments), and security testing (vulnerability identification and prevention of attacks).

- Thoroughness is measured by coverage of non-functional elements.

- Specialized skills may be needed (performance or security testing).

### Change-Related Testing

- Addresses changes to software function and/or structure.

- Considers the change itself and its other effects.

- Confirmation testing (re-testing) confirms that defects are fixed and don't cause new failures.

- Regression testing ensures that changes haven't introduced new defects or uncovered existing ones in unchanged areas.

- Both are done at all test levels.

## Test Types and Test Levels

### Test Levels

- Groups of test activities organized and managed together.

- Each level has specific objectives, a test basis, a test object, typical defects, approaches, responsibilities, and a test environment.

### Test Levels and Their Characteristics

- Component testing (module or unit testing): tests individual components.

- Integration testing: tests interfaces and interactions between components or systems.

- System testing: verifies that the whole system meets requirements.

- Acceptance testing: validates the system against user needs, requirements, and business processes.

### Different Forms of Acceptance Testing

- User acceptance testing (UAT): conducted in a real or simulated environment by intended users.

- Operational acceptance testing (OAT): performed in an operational environment by operations staff.

- Contractual acceptance testing: verifies compliance with contractual requirements.

- Regulatory acceptance testing: verifies compliance with laws and regulations.

- Alpha testing: simulated or actual testing in the developer's environment by external roles.

- Beta testing: simulated or actual testing at an external site by external roles.

### Test Level Characteristics Summary

- Objectives, test basis, test objects, and typical defects and failures are summarized for each test level (component, integration, system, acceptance).

### Test Types and Levels

- Each test type (functional, non-functional, white-box) is applicable at every test level.  An example using a banking application illustrates this.

## Static vs Dynamic Testing

### Static Testing

- Tests software without code execution.

- Reviews code, requirements, and design documents to find errors.

- Improves quality by finding early errors.

- Techniques include informal reviews, technical reviews, walkthroughs, inspections, and static code reviews.

### Dynamic Testing

- Executes code.

- Checks functional behavior, memory/CPU usage, and performance.

- Confirms that the software works according to requirements.

- Techniques include unit testing, integration testing, and system testing.

### Key Differences Between Static and Dynamic Testing

- Static testing is done without execution; dynamic testing involves execution.

- Static testing focuses on defect prevention; dynamic testing focuses on finding and fixing defects.

- Static testing performs verification; dynamic testing performs validation.

- Static testing happens before compilation; dynamic testing happens after compilation.

- Static testing techniques are structural and statement coverage-based; dynamic testing techniques include boundary value analysis and equivalence partitioning.

## Summary

### There is no single "one size fits all" testing process.

### Test activities may overlap or occur concurrently.

### Different testing methodologies exist (white-box, black-box, grey-box, functional, non-functional).

