# Software Testing Life Cycle (STLC)

## Requirements Analysis/Design

### High-Level Requirement Analysis

- This phase begins when the Software Requirements Document is ready and shared with stakeholders.

- The testing team identifies testing requirements, including scope, verification and validation key points, and time estimations.

- It's crucial to identify the expected timeframe for each functionality.

### Scope Definition

- Defines the scope of testing at a high level, breaking it down into several functional modules.

- Identifies the needed types of testing.

- Analyzes and identifies prerequisites and testing environment details.

### Requirement Traceability Matrix (RTM) Preparation

- Requirement tracing documents the links between requirements and work products developed to implement and verify those requirements.

- The RTM maps and traces user requirements with test cases.

- This ensures traceability and helps in managing requirements throughout the testing process.

### Automation Analysis

- Analyzes the scope of automation for regression testing.

- Plans the automation tool, functionalities to be automated, timeframe, and resource allocation.

- This helps in identifying areas where automation can improve efficiency and reduce testing time.

## Entry and Exit Criteria

### Entry Criteria

- Conditions or documents needed to start a specific STLC phase.

- Tasks are only performed if these conditions are met.

- Defining a timeframe for entry criteria is ideal.

### Common Entry Criteria

- Requirements are defined and approved.

- Complete or partially testable code is available.

- Test data and test cases are available.

- The testing environment is completely set up.

### Common Entry Criteria for Unit Testing

- The planning phase is completed.

- System design, technical design, and other relevant documents are reviewed and approved.

- Business and functional requirements are defined and approved.

- Testable codes or units are available.

- The test environment is available.

### Exit Criteria

- Items, documents, actions, or tasks that must be completed before concluding an STLC phase and moving to the next.

- A set of expectations for a particular phase.

- Defining a timeframe for exit criteria is ideal.

### Common Exit Criteria

- Deadlines are met.

- All test cases are executed.

- Sufficient coverage of requirements and functionalities is achieved.

- All identified defects are corrected and closed.

- No critical bugs are left unresolved.

### Common Exit Criteria for Unit Testing

- Unit tests are successfully executed.

- All identified bugs are fixed and closed.

- The project code is completed.

## Test Planning

### Defining Test Specifications

- Defines test specifications to meet project requirements.

- The main objective is to prepare a Test Plan document.

- This document outlines the testing strategy, resources, environment, limitations, and schedule.

### Test Plan Document

- Usually prepared by the Quality Assurance Team Lead or Manager.

- Focuses on data and information related to the application.

- Answers key questions like "What is to be tested?" and "What resources are required?".

### Entry and Exit Criteria for Test Planning

- Entry criteria include providing requirements documents along with the RTM and an automation feasibility report (if automation is in scope).

- Exit criteria include completing the Test Plan Document and Test Effort Estimation document.

- Major aspects include scope of deliverables, effort estimation, and resource planning.

## Test Case Design and Development

### Test Case Preparation

- The main objective is to prepare test cases for individual units.

- Test cases cover functionality, verification, and validation points mentioned in the Test Plan.

- They are written by the QA team and signed off by a Business Analyst after review.

### Entry Criteria for Test Case Development

- Activities in Test Planning are finished.

- The Test Plan is ready.

### Exit Criteria for Test Case Development

- Test cases are signed off.

- Test data is ready.

- Test scripts are prepared if automation is in scope.

### Activities in Test Case Development

- Test Scenarios Identification:  Simplifies testing and evaluation of complex systems. Strategies include enumerating possible users and their actions, evaluating users with a hacker's mindset, listing system events and how the system handles them, and studying similar systems and their behavior.

- Test Cases Writing: A test case includes test data, preconditions, expected results, and post-conditions.  It verifies compliance against specific requirements and serves as the starting point for test execution. Good test cases are simple, transparent, have 100% coverage, are identifiable, align with client expectations, and are regularly revised and updated.

- Test Data Preparation: Used to execute tests on the test platform. Data should be precise and exhaustive to uncover defects. It can be created manually or using automation methods and should have high coverage and be regularly updated.

## Test Environment Setup

### Test Environment

- Contains elements supporting test execution, including software, hardware, and network configurations.

- It's a combination of hardware and software environments for executing tests.

- This phase includes hardware configuration, operating system settings, software configuration, test terminals, and other support.

### Activities in Test Environment Setup

- Design Test Environment: Checks if archiving is needed for backups, verifies network configuration, identifies required server operating systems, databases, and other components, and identifies the number of licenses needed.

- Environment Set Up: Analyzes setup requirements, prepares software and hardware requirements, and gets official confirmation for the test environment setup.

- Smoke Testing: Validates the test environment's stability and prevents wasting time and resources on deeper tests with a broken product.

## Defect Life Cycle

### States of a Defect Life Cycle

- New: A potential defect raised but not yet validated.

- Assigned: Assigned to a development team for checking.

- Active: The defect is being addressed by the developer. Possible outcomes are deferred or rejected.

- Test/Fixed/Ready for Retest: The defect is fixed and ready for retesting.

- Verified: The defect is retested and verified by QA.

- Closed: The final state; closed after QA retesting or if it's a duplicate or not a defect.

- Reopened: An unfixed defect reactivated or reopened by QA.

- Deferred: A defect that cannot be addressed in the current cycle and is deferred to a future release.

- Rejected: A defect rejected because it's a duplicate, not a defect, or non-reproducible.

## Defects Classification

### Priority and Severity

- QA team perspective: Priority (order of resolving defects).

- Development perspective: Severity (complexity of fixing the code).

- QA sets priority and timeframe based on end-user requirements.

### Priority Categories

- Low: Defects fixed after critical defects.

- Medium: Defects resolved in subsequent builds.

- High: Defects solved immediately due to considerable impact on application functionality.

- Urgent: Defects resolved immediately due to severe impact, rendering the product unusable.

### Severity Categories

- Severity is the impact of the defect on the application and the complexity of fixing it from a development perspective.

- It's determined by how crucial the defect is to the system.

- Severity status indicates the deviation in functionality due to the defect.

### Severity Listings

- Critical/Severity 1: Impacts crucial functionality; QA cannot continue validation without fixing it (e.g., frequent app crashes).

- Major/Severity 2: Impacts a functional module; QA can continue validating other modules (e.g., flight reservation not working).

- Medium/Severity 3: Issue with a single screen or function; system still functions (e.g., ticket number format incorrect).

- Low/Severity 4: Does not impact functionality; cosmetic defect or UI inconsistency (e.g., mismatched button colors).

## Test Execution

### Objectives

- Ensure no bugs or issues exist in the application; it's "fit for purpose".

- Unit and integration testing are performed, comparing expected and actual results.

- Identified defects are corrected until all tests pass.

### Activities in Test Execution

- Testers execute tests, log defects, and communicate progress to the test manager.

- Test status is communicated to management; the development team resolves defects and produces new releases.

- The testing team retests failed test blocks and performs regression testing to confirm main functionality is maintained.

### Entry Criteria for Test Execution

- Test Plan is complete.

- Test Cases Development Phase is complete.

- Test data is available.

### Exit Criteria for Test Execution

- All test cases are successfully validated.

- Defects are deferred.

- Test case execution is complete.

- A Defect Summary Report is available.

### Main Activities in Test Execution

- System Integration Testing (SIT): A black box testing technique evaluating system compliance against requirements/test cases. Usually performed on a subset of the system.  Can be performed with minimal testing tools; interactions and data field behavior are investigated. After integration, data flow is checked in the integration, database, and application layers.

- Defect Reporting: A process of finding defects by testing or recording feedback; new versions fix defects based on client feedback. Managing, evaluating, and prioritizing defects is challenging.

- Defect Mapping: Once a defect is reported and logged, it's mapped with failed/blocked test cases and corresponding requirements in the RTM. This helps create proper defect reports and analyze impact. Stakeholders decide whether to fix or defer based on priority and severity.

- Re-testing: Executing a previously failed test after a defect is fixed to check if the problem is resolved.

- Regression Testing: Re-executing tests impacted by code changes. Types include final regression tests (validating unchanged builds) and regression tests (verifying that code changes haven't broken other parts of the application).

## Test Closure

### Final Phase

- The final phase of the STLC.

- Completeness is ensured by checking against test exit criteria and product quality measurement against test completion criteria.

### Entry Criteria for Test Closure

- Test case execution is complete.

- Test results are available.

- Defects report is available.

### Exit Criteria for Test Closure

- Test closure reports are provided.

- Matrices are prepared (to be signed by clients).

### Main Activities in Test Closure

- Test Completion Reporting: Summarizes test metrics to update stakeholders, enabling informed decisions. The report includes the test summary report identifier, summary, variances, summary results, evaluation, planned vs. actual efforts, and sign-off.

- Test Completion Matrix Reporting: Upon completion, various matrices are collected to prepare test reports.

- Availability of Test Results: Articulates test results (screenshots, database query results, recordings, log files).  These are saved during test case execution, re-testing, and regression testing, and included in test closure documents.

